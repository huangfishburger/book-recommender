# BookRec Runner & Evaluator

This project provides two main functional threads:

1. **Tag Generation**: Uses GPT to generate 15–20 Traditional Chinese tags based on book title/author/summary, outputting the results to an Excel file.

2. **Retrieval and Evaluation**:

**Terms to Books**: Uses the tag vector store for retrieval. Scores each term against the candidate book's **summary** using **BM25** or **Cosine Similarity**, then aggregates the scores (avg/sum/min).

**Books to Books**: Finds similar books based on the source book's (**tags** or **summary**). Calculates BM25 and Cosine Similarity (with optional length normalization) between the "source query" and the candidate book's **summary**. Also calculates Cosine Similarity between the "source query" and the candidate book's **title** and **category**.

**Tag Quality Evaluation**: Assesses the relevance of "each book's tags vs. its own book summary" using BM25 and Cosine Similarity (`evaluation.py`).

All executions are controlled via a single script file: bookrec/script.py. The script outputs the results, two summary tables (for terms/books), and execution parameters + runtime (meta.json), facilitating reproducibility and comparison.

所有執行都可用單檔 **`bookrec/script.py`** 控制；會輸出**結果**、**兩張彙整表（terms/books）**與**執行參數＋耗時（meta.json）**，方便重現與比較。

All executions are controlled via a single script file: **`bookrec/script.py`**. The script outputs the **results**, **two summary tables (for terms/books)**, and **execution parameters + runtime (meta.json)**, facilitating reproducibility and comparison.

---

## 0. Requirements and Installation

**Python**：3.9+

**套件**：

* Required: `pandas`, `numpy`, `faiss-cpu`, `openpyxl`, `python-dotenv`, `tqdm`
* Optional:`jieba`(for better Chinese tokenization),`rich`(for nicer CLI output)

```bash
pip install -U pandas numpy faiss-cpu openpyxl python-dotenv tqdm jieba
```

**OpenAI API Key (If tag generation is needed)**：

* Set the environment variable `OPENAI_API_KEY`, or place a `.env` file in the project root:

  ```env
  OPENAI_API_KEY=sk-************************
  ```

**Default Paths**：

```
data/process/processed_books.csv   # Raw book catalog (Title/Author/Summary/Second-level Category)
data/process/title_tags.xlsx     # Tag output (Title/Category/Tags)
data/process/                      # Vector index, cache, and evaluation output
```

> **Recommendation**: Run as a module (to ensure bookrec is findable):
>
> ```bash
> python -m bookrec.script ...
> ```

---

## 1. File Overview

* `data/code/add_tags.py`: Tag generation (`batch_generate_labels()`).
* `bookrec/embeddings.py`: `EmbeddingClient` (OpenAI Embeddings).
* `bookrec/core.py`:

  * `VectorStore`: Reads Excel tags, builds/loads FAISS, inverted index, and IDF.
  * `SearchService`:

    * `terms_to_books(query_input, ...)` (**Terms to Books**)
    * `books_to_books(titles_input, ...)` (**Books to Books**)
* `data/code/evaluation.py`: BM25, cosine similarity tools, and tag evaluation (`BM25`, `BM25Params`, `tokenize`, `evaluate`).
* `bookrec/script.py`: Single file integration (optional tag regeneration → optional tag evaluation → build/load vector store → terms-to-books / books-to-books + BM25 + cosine similarity → outputs results, meta, & summary tables).

---

## 2. Input Format

### `processed_books.csv`

Required fields:

* `title`（str，唯一鍵）
* `author`（str）
* `summary`（str）
* `category`（str）

### `title_tags.xlsx`

Generated by `add_tags.py` or `batch_generate_labels()`:

* `title`（str）
* `category`（str）
* `tags`（str, comma-separated,e.g., `growth, psychology`）

---

## 3. Quick Start

### 3.1 Run Terms to Books Directly (No Tag Regeneration)

```bash
python -m bookrec.script \
  --excel data/process/title_tags.xlsx \
  --books-csv data/process/processed_books.csv \
  --terms "magic, science" \
  --topk 10
```

Output:

* Top-K results for each query printed to the terminal (Vector retrieval score + **per-term aggregated BM25** score + cosine similarity score).
* Result files are in the `experiment/current_date` folder.
* **`results.jsonl`**: One JSON object per line (retrieval results).
* **`results_terms.csv`**: Summary of Top-K results for all "Terms to Books" queries (fields include `query, rank, title, category, score, bm25_score, cosine_score, exact_hits, similar_hits`).
* **`results.meta.json`**: Start/end time, elapsed time, all parameters, and summary table paths.

### 3.2 Regenerate Tags and Perform BM25 Tag Evaluation

```bash
python -m bookrec.script \
  --regen-tags true \
  --gpt-model gpt-4o \
  --books-csv data/process/processed_books.csv \
  --excel data/process/title_tags.xlsx \
  --eval-tags-bm25 true \
  --eval-out-csv data/process/bm25_eval_summary.csv \
  --eval-out-jsonl data/process/bm25_eval_details.jsonl
```

Output:

* Generates/overwrites `title_tags.xlsx`.
* Tag BM25 Evaluation: `bm25_eval_summary.csv` (per-book summary), `bm25_eval_details.jsonl` (per-tag details).
* If queries/books are also provided, `results.jsonl`, `results_terms.csv`/`results_books.csv`, and `results.meta.json` will be generated additionally.

### 3.3 Regenerate Tags and Perform Cosine Similarity Tag Evaluation

```bash
python -m bookrec.script \
  --regen-tags true \
  --gpt-model gpt-4o \
  --books-csv data/process/processed_books.csv \
  --excel data/process/title_tags.xlsx \
  --eval-tags-cosine true \
  --eval-out-csv data/process/cosine_eval_summary.csv \
  --eval-out-jsonl data/process/cosine_eval_details.jsonl
```

Output:

* Generate/overwrite `title_tags.xlsx`
* Tag Cosine Similarity Evaluation: `cosine_eval_summary.csv` (per-book summary), `cosine_eval_details.jsonl` (per-tag details)
* If queries/books are also provided, additional files will be generated: `results.jsonl`, `results_terms.csv` / `results_books.csv`, and `results.meta.json`

### 3.4 books-to-books Recommendation (BM25 computed using source book tags)

```bash
python -m bookrec.script \
  --excel data/process/title_tags.xlsx \
  --books-csv data/process/processed_books.csv \
  --books "The Courage to Be Disliked; Atomic Habits" \
  --bookq-mode-bm25 tags \
  --bookq-merged-tf-bm25 binary \
  --bookq-normalize-bm25 true \
  --bookq-mode-cosine tags \
  --embedding-level-books sentence-max \
  --bookq-normalize-cosine true \
  --bookq_title_sim true \
  --bookq_category_sim true \
  --topk 10
```

Output (besides terminal logs and `results.jsonl`):

* Result files are stored under the `experiment/<date>` folder
* **`results_books.csv`**: View the Top-K results for all source books in the “books-to-books” mode (columns include `source_title, bookq_mode_bm25, bookq_mode_cosin e, rank, title, category, score, source_hits, match_count, bm25_score, cosine_score, [title_sim, category_sim, bm25_norm, cosine_norm]`)

### 3.5 books-to-books Recommendation (BM25 computed using source book summary)

```bash
python -m bookrec.script \
  --excel data/process/title_tags.xlsx \
  --books-csv data/process/processed_books.csv \
  --books "Harry Potter" \
  --bookq-mode-bm25 summary \
  --bookq-mode-cosine summary \
  --embedding-level-books sentence-max \
  --bookq_title_sim true \
  --bookq_category_sim true \
  --topk 10
```

> If `--books` / `--books-file` is not provided, or if the source books cannot be matched, `results_books.csv` will not be generated; you can check the actual path (or whether it is an empty string) under `books_csv` in `results.meta.json`.

---

## 4. Parameter Description (with Defaults)

### 4.1 Tag Generation (Optional)

* `--regen-tags` (`false`): Whether to regenerate tags (calls OpenAI)
* `--gpt-model` (`gpt-4o`): GPT model
* `--books-csv` (`data/process/processed_books.csv`)
* `--regen-range` (e.g., `1100:-1`): Specify index range
* `--save-every` (`500`): Checkpoint frequency
* `--excel` (`data/process/title_tags.xlsx`): Tag output target
* **Configurable Prompt**:

  * `--system-prompt`: System instruction (default: *Your main task is to provide topic tags for each book based on its content.*)
  * `--prompt-inline`: Pass user prompt template directly via CLI (supports `{title} {author} {summary}` placeholders)
  * `--prompt-file`: Load template from file (UTF-8), also supports `{title} {author} {summary}`
  * `--temperature`: Temperature for Chat Completions (default 0.3)

> **Prompt Source Logging**: In `results.meta.json`, the field `prompt_source` is labeled as `default` / `inline` / `file:PATH`.

### 4.2 Embedding Store & Retrieval

* `--embedding-model` (`text-embedding-3-small`)
* `--rebuild-index` (`false`): Whether to rebuild the vector store (otherwise load existing)
* `--sim-th` (`0.55`): Similar-tag threshold
* `--tag-topk` (`20`): Similar-tag Top-K per term
* `--topk` (`10`): Final Top-K books returned
* `--mode` (`soft_and`): Aggregation mode for book scoring (`sum` / `avg` / `min` / `soft_and`)
* `--use-idf` (`false`): Whether to multiply similar-tag weights by tag IDF
* `--cooccur-bonus` (`0.2`): Bonus for multi-term co-occurrence under `soft_and` mode

### 4.3 Terms-to-Books Search (Input + BM25 Aggregation)

* `--terms`: Multiple queries separated by semicolons; each query split into terms by commas (supports full-width/half-width)
* `--terms-file`: One query per line (same format)
* **BM25 Scoring Method**: The query is split into multiple **terms**, **each term** computes BM25 against candidate book **summaries**, then aggregated using:

  * `--terms-bm25-agg {avg|sum|min}` (default `avg`)

> Terms-to-books BM25 also uses `--bm25-*` parameters (tokenizer/ngram/k1/b).

### 4.4 books-to-books Search (Input + Evaluation)

* `--books` / `--books-file`: Source book titles (semicolon-separated or one per line)
* `--bookq-mode-bm25` (`tags|summary`, default `tags`):
* `--bookq-mode-cosine` (`tags|summary`, default `tags`):

  * `tags`: Use all tags of the source book combined as a single query  
  * `summary`: Use the source book's **summary** as the query

* `--bookq-merged-tf-bm25` (`binary|log|raw`, default `binary`): Only applies under `tags` mode

  * `binary`: Token counted once
  * `log`: Frequency f → repeated ⌈log(1+f)⌉ times
  * `raw`: Preserve raw token frequency

* `--bookq-normalize-bm25` (`true|false`, default `true`): Whether to divide BM25 score by query length (prevents long queries from dominating)
* `--bookq-normalize-cosine` (`true|false`, default `true`): Whether to normalize cosine similarity by query length

### 4.5 BM25 (Shared Parameters: Evaluation & Retrieval Scoring)

* `--bm25-tokenizer` (`auto|jieba|whitespace|char`, default `auto`)

  * `auto`: Use `jieba` if installed; otherwise fallback to **character n-gram** for Chinese
* `--bm25-ngram` (`2`): n value for `char`/fallback mode (recommended 2 for Chinese)
* `--bm25-k1` (`1.2`), `--bm25-b` (`0.75`): BM25 parameters
* `--bm25-topk-mean` (`5`): Top-K average in **tag-each** mode (useful for measuring tag representativeness)

### 4.6 Cosine Similarity (Shared Parameters: Evaluation & Retrieval Scoring)

* `--embedding-level` (`sentence-max|sentence-avg`, default `sentence-max`): Represent overall similarity by max or average across sentence similarities
* `--cosine-topk-mean` (`5`): Top-K average in **tag-each** mode (useful for measuring tag representativeness)

### 4.7 Tag Quality Evaluation (Optional)  (with BM25 and Cosine Similarity)

* `--eval-out-csv`: Summary table (one row per book)
* `--eval-out-jsonl`: Detailed file (one row per book, including scores for each tag)

---

## 5. Output Description

### 5.1 `results.jsonl`

Each line is a JSON record:

* **Terms-to-Books**:

  ```json
  {
    "category": "terms_to_books",
    "query": "Fantasy, Science",
    "results": [
      {"title": "...", "category": "...", "tags": ["..."], "score": 3.27, "exact_hits": 1, "similar_hits": 5, "bm25_score": 1.482, "cosine_score": 0.393},
      ...
    ]
  }
  ```
* **Books-to-Books**：

  ```json
  {
    "category": "books_to_books", 
    "source_title": "Harry Potter", 
    "bookq_mode_bm25": "tags", 
    "bookq_mode_cosine": "tags", 
    "results": [
      {"title": "...", "category": "...", "score": 5.506, "source_hits": 1, "match_count": 8, "title_sim": 0.332112, "category_sim": 0.32942, "bm25_score": 30.789589, "cosine_score": 0.551491, "bm25_norm": 0.789477, "cosine_norm": 0.014141}
      ...
    ]
  }
  ```

### 5.2 `results_terms.csv` & `results_books.csv`

* Result files are located in the `experiment/<date>` folder.
* **`results_terms.csv`**: Summary of all terms-to-books queries. Columns:  
  `query, rank, title, category, score, bm25_score, cosine_score, exact_hits, similar_hits`.
* **`results_books.csv`**: Summary of all books-to-books queries. Columns:  
  `source_title, bookq_mode_bm25, bookq_mode_cosine, rank, title, category, score, source_hits, match_count, bm25_score, cosine_score, [title_sim, category_sim, bm25_norm, cosine_norm]`.

> If there are no books-to-books output rows, `results_books.csv` will not be generated.  
> The actual file path can be checked in the `books_csv` field inside `results.meta.json`.

### 5.3 `results.meta.json`

* `start_time`, `end_time`, `elapsed_sec`
* `args`: All CLI arguments (for reproducibility)
* `prompt_source`: `default` / `inline` / `file:PATH`
* `terms_csv`, `books_csv`: Actual paths for the two summary tables (empty string if not generated)
* If `--eval-tags-bm25 true` was executed: `bm25_eval_summary_csv`, `bm25_eval_details_jsonl`
* If `--eval-tags-cosine true` was executed: `cosine_eval_summary_csv`, `cosine_eval_details_jsonl`

### 5.4 Tag BM25 Evaluation Output (when `--eval-tags-bm25 true`)

* `bm25_eval_summary.csv` (one row per book):

  * Example columns:  `title, tag_count, TE_avg, TE_max, TE_max_tag, TE_coverage, TE_top5_mean, TM_score, TM_len, TM_norm, TM_tf, TM_norm_on`

* `bm25_eval_details.jsonl` (one line per book):

  * `{ "title": "xxx", "scores": [{"tag": "Adler Psychology", "bm25": 1.234}, ...], "tag_merged_score": 3.21, ... }`

### 5.5 Tag Cosine Similarity Evaluation Output (when `--eval-tags-cosine true`)

* `cosine_eval_summary.csv` (one row per book):

  * Example columns:  `title, tag_count, TE_avg, TE_max, TE_max_tag, TE_top5_mean, TM_score, TM_len, TM_norm, TM_norm_on`

* `cosine_eval_details.jsonl` (one line per book):

  * `{ "title": "xxx", "scores": [{"tag": "Adler Psychology", "cosine": 1.234}, ...], "tag_merged_score": 3.21, ... }`

---

## 6. Algorithm Tips

* **Vector Retrieval**:  
  Query (terms or source book tags) → find similar tags (FAISS) → aggregate by book (optional IDF/mode/co-occurrence bonus) → Top-K books.

* **BM25 (document side is always the candidate book "summary")**:

  * **Terms-to-Books**: Split the query by commas into multiple terms, compute BM25 **per term** against book summaries, then aggregate using `--terms-bm25-agg` (default `avg`).
  * **books-to-books**: Query depends on `--bookq-mode-bm25`:

    * `tags`: Source book tags merged (`binary/log/raw`), with optional `--bookq-normalize-bm25`.
    * `summary`: Source book full summary.
    * When multiple books are queried, the final score is the average across all source books.

* **Cosine Similarity (document side is always the candidate book "summary", split into sentences)**:

  * **Terms-to-Books**: Split the query into terms by commas, compute scores using the **average embedding vector of all terms**, and compare against each sentence of the candidate summary; final score uses `--embedding-level-books` (default `sentence-max`).
  * **books-to-books**: Query depends on `--bookq-mode-cosine`:

    * `tags`: Source book tags merged (`binary/log/raw`), with optional `--bookq-normalize-cosine`.
    * `summary`: Source book full summary, sentence-splitted.
    * When multiple books are queried, the final score is the average across all source books.

---

## 7. Frequently Asked Questions (FAQ)

* **ModuleNotFoundError: bookrec?**  
  Run the script from the repo root using module mode:  `python -m bookrec.script ...`

* **Excel and vector store not synced?**  
  If you regenerate or modify the tag Excel and the tag count mismatches when loading, add `--rebuild-index true` to rebuild the vector store.

* **Why is `results_books.csv` missing?**  
  It will not be generated if `--books` / `--books-file` is not provided, the source title does not match, or no rows are returned.  Check the `books_csv` field in `results.meta.json`.

* **BM25 tokenization suggestions?**  
  Install `jieba`.  If not available, `auto` falls back to Chinese character n-grams; recommended setting: `--bm25-ngram 2`.

* **Looking for one representative metric?**  
  Use `TE_top{k}_mean` (default k=5).  It is more stable than `TE_max` and more representative than simple averaging.

---

## 8. Example Workflow

**Full pipeline (Regenerate Tags → Tag Evaluation → Terms-to-Books & books-to-books Search)**

```bash
python -m bookrec.script \
  --regen-tags true --gpt-model gpt-4o \
  --books-csv data/process/processed_books.csv \
  --excel data/process/title_tags.xlsx \
  --rebuild-index true \
  --eval-tags-bm25 true \
  --eval-tags-cosine true \
  --eval-out-csv-bm25 data/process/bm25_eval_summary.csv \
  --eval-out-jsonl-bm25 data/process/bm25_eval_details.jsonl \
  --eval-out-csv-cosine data/process/cosine_eval_summary.csv \
  --eval-out-jsonl-cosine data/process/cosine_eval_details.jsonl \
  --terms "Psychology, Growth; Investment, Financial Report" \
  --books "Harry Potter" \
  --bookq-mode-bm25 tags --bookq-merged-tf-bm25 binary --bookq-normalize-bm25 true \
  --bookq-mode-cosine tags --embedding-level-books sentence-max --bookq-normalize-cosine true \
  --topk 10
```

## Appendix
Explanation of book tag quality evaluation metrics  (generated by the `evaluate` function for BM25 and Cosine Similarity)

| Metric           | Description                                      |
|------------------|------------------------------------------------------|
| **TE_avg**       | Average score across all tags; reflects overall relevance of tags to content. |
| **TE_max**       | Highest score among all tags; represents the most relevant tag.               |
| **TE_max_tag**   | The name of the tag with the highest score; helps identify the most representative tag. |
| **TE_coverage**  | Ratio of tags with score > 0; measures effective tag coverage.                |
| **TE_topK_mean** | Average of the top K highest-scoring tags (default K=5), avoiding outlier influence. |
| **TM_score**     | Total BM25 score after merging all tags.  |
| **TM_len**       | Token count of the merged query, reflecting total tag length. |
| **TM_norm**      | Normalized score = `TM_score / TM_len` (if normalize_query is enabled).  |
| **TM_tf**        | Token frequency strategy used when merging tags (e.g., `binary` counts presence only). |
| **TM_norm_on**   | Whether normalization is enabled (boolean), determining if TM_norm is length-adjusted. |

> Output formats:  
> - **CSV**: Summary table  
> - **JSONL**: Per-book record including detailed tag scores
